{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb52574b-738c-4925-9bbb-b39297932d89",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f5a0ad-ec17-47d4-8107-e5edb582e8b3",
   "metadata": {},
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import models, transforms\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8f712b0b-8f9c-4779-aff8-36e793015083",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "988c39fe-f956-4dbe-b33a-74795c379563",
   "metadata": {},
   "source": [
    "data_path = '/Users/maksimtrebusinin/Downloads/flowers'\n",
    "\n",
    "class_counts = dict()\n",
    "\n",
    "for class_name in os.listdir(data_path):\n",
    "    class_dir = os.path.join(data_path, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        class_counts[class_name] = len(os.listdir(class_dir))\n",
    "\n",
    "print('Распределение классов:')\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f'{class_name} - {count}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01bbe382-6847-4f81-8ac0-08e27e8d7621",
   "metadata": {},
   "source": [
    "#Создаем датафрейм с путями к изображениям и их лейблами\n",
    "\n",
    "data = []\n",
    "for class_name in os.listdir(data_path):\n",
    "    class_dir = os.path.join(data_path, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            data.append((os.path.join(class_dir, img_name), class_name))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['image_path', 'label'])\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "print(f'Длина тренировочной выборки: {len(df_train)}')\n",
    "print(f'Длина тестовой выборки: {len(df_test)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0705ff16-105a-4584-af47-99d1bb3095a1",
   "metadata": {},
   "source": [
    "print(df_train['label'].unique())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e5e55d4-7a15-46df-9f59-68d90c5baef9",
   "metadata": {},
   "source": [
    "df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=42, stratify=df_train['label'])\n",
    "label_encoder = LabelEncoder()\n",
    "df_train[\"label\"] = label_encoder.fit_transform(df_train[\"label\"])\n",
    "df_val[\"label\"] = label_encoder.transform(df_val[\"label\"])\n",
    "\n",
    "print(df_train.head(2))\n",
    "print(df_val.head(2))\n",
    "# Класс для кастомного датасета\n",
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image_path = self.dataframe.iloc[idx][\"image_path\"]\n",
    "            label = self.dataframe.iloc[idx][\"label\"]\n",
    "\n",
    "            # Загрузка изображения\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            # Преобразования\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "# преобразования для обучения\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# преобразования для валидации\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# создание датасетов\n",
    "train_dataset = FlowerDataset(df_train, transform=train_transform)\n",
    "val_dataset = FlowerDataset(df_val, transform=val_transform)\n",
    "\n",
    "# создание загрузчиков данных\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a1ac6431-a468-41b4-9ba0-ae0c2ba132f2",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c67a90c3-c7a3-4ddd-bca6-77bd074e07a3",
   "metadata": {},
   "source": [
    "# Загрузка ResNet50\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Замена последнего слоя\n",
    "num_classes = len(df_train[\"label\"].unique())\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=6):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Обучение\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Статистика\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100.0 * correct_train / total_train\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100.0 * correct_val / total_val\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss / len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9b2f5a8c-de49-4196-8ee6-beabe3d801dd",
   "metadata": {},
   "source": [
    "# Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a61e020c-ffcf-400b-8995-32064de24015",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_embedding(image_path, model, transform):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image).squeeze().numpy()\n",
    "    return embedding\n",
    "\n",
    "ex_path = '/Users/maksimtrebusinin/Downloads/flowers/tulip/9446982168_06c4d71da3_n.jpg'\n",
    "ex_emb = get_embedding(ex_path, model, transform)\n",
    "print(f'Размер эмбеддинга: {ex_emb.shape}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cbfb4f2-b5f1-406e-9f73-72f51e0e33b5",
   "metadata": {},
   "source": [
    "#Сохранение эмбеддингов для тествой выборки\n",
    "\n",
    "embeddings = {}\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    image_path = row['image_path']\n",
    "    embedding = get_embedding(image_path, model, transform)\n",
    "    embeddings[image_path] = embedding\n",
    "\n",
    "with open(\"test_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "print(\"Эмбеддинги сохранены.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "279b9c94-8b4d-448e-8b36-2bfd4fd6a9f5",
   "metadata": {},
   "source": [
    "def calculating_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity([embedding1], [embedding2])[0][0] #Вычиляем косинусово сходство\n",
    "\n",
    "def find_top_similar_images(entrance_embedding, test_embeddings, top_k = 5):\n",
    "    similarities = []\n",
    "    for image_path, embedding in test_embeddings.items():\n",
    "        similarity = calculating_similarity(entrance_embedding, embedding)\n",
    "        similarities.append((image_path, similarity))\n",
    "        \n",
    "#Лист со скорами сходства сортируется и возвращается топ k первых элементов\n",
    "    similarities.sort(key = lambda x: x[1], reverse = True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "#Пример использования\n",
    "inp_image = '/Users/maksimtrebusinin/Downloads/flowers/dandelion/9965757055_ff01b5ee6f_n.jpg'\n",
    "emb_of_inp_image = get_embedding(inp_image, model, transform)\n",
    "top_similar_images = find_top_similar_images(emb_of_inp_image, embeddings)\n",
    "for path, similar in top_similar_images:\n",
    "    print(f'{path}: {similar:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ace85fb3-450f-4908-a0b1-4cdec22c8537",
   "metadata": {},
   "source": [
    "# Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34857eb0-122a-4cd7-8be4-33fa8cd8c61d",
   "metadata": {},
   "source": [
    "def display_results(input_img_path, top_sim):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Отображение входного изображения\n",
    "    plt.subplot(1, 6, 1)\n",
    "    input_image = Image.open(input_img_path)\n",
    "    plt.imshow(input_image)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    for i, (image_path, similarity) in enumerate(top_sim):\n",
    "        plt.subplot(1, 6, i + 2)\n",
    "        similar_image = Image.open(image_path)\n",
    "        plt.imshow(similar_image)\n",
    "        plt.title(f\"Sim: {similarity:.2f}\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "display_results(inp_image, top_similar_images)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3217f0b0-e367-4e40-84a0-7c72fa2e32c8",
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "# Выбор случайных изображений\n",
    "random_test_images = random.sample(list(df_val[\"image_path\"]), 5)\n",
    "\n",
    "# Показ результатов\n",
    "for image_path in random_test_images:\n",
    "    input_embedding = get_embedding(image_path, model, transform)\n",
    "    top_similar_images = find_top_similar_images(input_embedding, embeddings)\n",
    "    display_results(image_path, top_similar_images)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7c4c8e92-9958-43b6-a095-bf01094237cf",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ede3d39-337b-4a95-816d-854bf1f815de",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958aadab-701a-4721-97cd-414290560d06",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
